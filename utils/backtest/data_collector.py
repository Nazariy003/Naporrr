# utils/backtest/data_collector.py
import time
import asyncio
import json
import pyarrow as pa
import pyarrow.parquet as pq
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from config.settings import settings
from utils.logger import logger


class BacktestDataCollector:
    """
    üéØ –ü—Ä–∞–≤–∏–ª—å–Ω–∏–π –∑–±—ñ—Ä –¥–∞–Ω–∏—Ö –¥–ª—è –±–µ–∫—Ç–µ—Å—Ç—É –∑ PyArrow ParquetWriter
    
    –†—ñ–≤–Ω—ñ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è:
    - RAW (7 –¥–Ω—ñ–≤): orderbook snapshots (5s) + trades + signals
    - AGGREGATED (30 –¥–Ω—ñ–≤): 1-min bars + aggregated signals
    - METADATA (90 –¥–Ω—ñ–≤): —Ç—ñ–ª—å–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Å–∏–≥–Ω–∞–ª—ñ–≤
    
    –í–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è:
    - –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è pq.ParquetWriter –∑–∞–º—ñ—Å—Ç—å pd.to_parquet –¥–ª—è append
    - PyArrow schemas –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
    - JSON serialization –¥–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä
    - –ü—Ä–∞–≤–∏–ª—å–Ω–µ –∫–µ—Ä—É–≤–∞–Ω–Ω—è writer lifecycle
    """
    
    def __init__(self, storage):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º–∏ PyArrow schemas"""
        self.storage = storage
        self.storage_path = Path(settings.backtest.data_storage_path)
        self.raw_path = self.storage_path / "raw"
        self.agg_path = self.storage_path / "aggregated"
        self.meta_path = self.storage_path / "metadata"
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ–π
        for path in [self.raw_path, self.agg_path, self.meta_path]:
            path.mkdir(parents=True, exist_ok=True)
        
        # –ë—É—Ñ–µ—Ä–∏ –¥–ª—è –±–∞—Ç—á–æ–≤–æ–≥–æ –∑–∞–ø–∏—Å—É
        self.buffers: Dict[str, Dict[str, List[Dict]]] = {
            'orderbook': {},
            'trades': {},
            'signals': {},
            'positions': {}
        }
        
        # ParquetWriter instances (–æ–¥–∏–Ω writer –Ω–∞ —Ñ–∞–π–ª –¥–ª—è append)
        self.writers: Dict[str, pq.ParquetWriter] = {}
        
        # Schemas –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ç–∏–ø—É –¥–∞–Ω–∏—Ö
        self.schemas = self._create_schemas()
        
        self.buffer_size = 100
        self.last_flush = time.time()
        self._running = False
        self._tasks = []
        
    def _create_schemas(self) -> Dict[str, pa.Schema]:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è PyArrow schemas –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó"""
        
        # Schema –¥–ª—è orderbook snapshots
        orderbook_schema = pa.schema([
            ('timestamp', pa.float64()),
            ('symbol', pa.string()),
            ('best_bid', pa.float64()),
            ('best_ask', pa.float64()),
            ('bid_levels', pa.string()),  # JSON string
            ('ask_levels', pa.string()),  # JSON string
            ('spread_bps', pa.float64()),
        ])
        
        # Schema –¥–ª—è trades
        trades_schema = pa.schema([
            ('timestamp', pa.float64()),
            ('symbol', pa.string()),
            ('price', pa.float64()),
            ('size', pa.float64()),
            ('side', pa.string()),
            ('is_aggressive', pa.bool_()),
        ])
        
        # Schema –¥–ª—è signals
        signals_schema = pa.schema([
            ('timestamp', pa.float64()),
            ('symbol', pa.string()),
            ('signal', pa.string()),
            ('strength', pa.int32()),
            ('composite', pa.float64()),
            ('imbalance', pa.float64()),
            ('momentum', pa.float64()),
            ('volatility', pa.float64()),
            ('settings_snapshot', pa.string()),  # JSON string
        ])
        
        # Schema –¥–ª—è closed positions
        positions_schema = pa.schema([
            ('timestamp', pa.float64()),
            ('closed_timestamp', pa.float64()),
            ('symbol', pa.string()),
            ('side', pa.string()),
            ('entry_price', pa.float64()),
            ('exit_price', pa.float64()),
            ('pnl', pa.float64()),
            ('close_reason', pa.string()),
            ('lifetime_sec', pa.float64()),
            ('stop_loss', pa.float64()),
            ('take_profit', pa.float64()),
        ])
        
        return {
            'orderbook': orderbook_schema,
            'trades': trades_schema,
            'signals': signals_schema,
            'positions': positions_schema,
        }
    
    async def start(self):
        """–ó–∞–ø—É—Å–∫ –∑–±–æ—Ä—É –¥–∞–Ω–∏—Ö"""
        logger.info("üé¨ [BACKTEST_DATA_COLLECTOR] Starting...")
        self._running = True
        
        # –ü—ñ–¥–ø–∏—Å–∫–∞ –Ω–∞ –ø–æ–¥—ñ—ó –ø–æ–∑–∏—Ü—ñ–π
        self.storage.add_position_callback(self._on_position_update)
        
        # –ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª—ñ—á–Ω–∏—Ö –∑–∞–¥–∞—á
        self._tasks = [
            asyncio.create_task(self._snapshot_loop()),
            asyncio.create_task(self._trades_loop()),
            asyncio.create_task(self._signals_loop()),
            asyncio.create_task(self._flush_loop()),
            asyncio.create_task(self._rotation_loop()),
        ]
        
        logger.info("‚úÖ [BACKTEST_DATA_COLLECTOR] Started successfully")
    
    async def stop(self):
        """–ó—É–ø–∏–Ω–∫–∞ –∑–±–æ—Ä—É –¥–∞–Ω–∏—Ö –∑ graceful shutdown"""
        logger.info("üõë [BACKTEST_DATA_COLLECTOR] Stopping...")
        self._running = False
        
        # –°–∫–∞—Å—É–≤–∞–Ω–Ω—è –≤—Å—ñ—Ö –∑–∞–¥–∞—á
        for task in self._tasks:
            task.cancel()
        
        # –û—á—ñ–∫—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è –∑–∞–¥–∞—á
        await asyncio.gather(*self._tasks, return_exceptions=True)
        
        # –§—ñ–Ω–∞–ª—å–Ω–∏–π flush –≤—Å—ñ—Ö –±—É—Ñ–µ—Ä—ñ–≤
        self._flush_all_buffers()
        
        # –ó–∞–∫—Ä–∏—Ç—Ç—è –≤—Å—ñ—Ö writers
        self._close_all_writers()
        
        logger.info("‚úÖ [BACKTEST_DATA_COLLECTOR] Stopped successfully")
    
    async def _snapshot_loop(self):
        """–ó–Ω—ñ–º–∫–∏ orderbook –∫–æ–∂–Ω—ñ N —Å–µ–∫—É–Ω–¥"""
        interval = settings.backtest.orderbook_snapshot_interval_sec
        
        while self._running:
            try:
                await asyncio.sleep(interval)
                
                for symbol in settings.pairs.trade_pairs:
                    ob = self.storage.get_order_book(symbol)
                    if not ob or not ob.bids or not ob.asks:
                        continue
                    
                    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ —Ç–æ–ø-10 —Ä—ñ–≤–Ω—ñ–≤ –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –º—ñ—Å—Ü—è
                    bid_levels = [(lvl.price, lvl.size) for lvl in ob.bids[:10]]
                    ask_levels = [(lvl.price, lvl.size) for lvl in ob.asks[:10]]
                    
                    spread_bps = ((ob.best_ask - ob.best_bid) / ob.best_bid) * 10000
                    
                    snapshot = {
                        'timestamp': ob.ts,
                        'symbol': symbol,
                        'best_bid': ob.best_bid,
                        'best_ask': ob.best_ask,
                        'bid_levels': json.dumps(bid_levels),  # JSON serialization
                        'ask_levels': json.dumps(ask_levels),
                        'spread_bps': spread_bps,
                    }
                    
                    self._add_to_buffer('orderbook', symbol, snapshot)
                    
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"‚ùå [SNAPSHOT_LOOP] Error: {e}")
    
    async def _trades_loop(self):
        """–ó–±—ñ—Ä trades –∫–æ–∂–Ω—ñ N —Å–µ–∫—É–Ω–¥"""
        interval = settings.backtest.trades_collection_interval_sec
        
        while self._running:
            try:
                await asyncio.sleep(interval)
                
                for symbol in settings.pairs.trade_pairs:
                    trades = self.storage.get_trades(symbol)
                    if not trades:
                        continue
                    
                    for trade in trades:
                        trade_data = {
                            'timestamp': trade.ts,
                            'symbol': symbol,
                            'price': trade.price,
                            'size': trade.size,
                            'side': trade.side,
                            'is_aggressive': trade.is_aggressive
                        }
                        self._add_to_buffer('trades', symbol, trade_data)
                        
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"‚ùå [TRADES_LOOP] Error: {e}")
    
    async def _signals_loop(self):
        """–ó–±—ñ—Ä —Å–∏–≥–Ω–∞–ª—ñ–≤ –∫–æ–∂–Ω—ñ N —Å–µ–∫—É–Ω–¥"""
        interval = settings.backtest.signals_collection_interval_sec
        
        while self._running:
            try:
                await asyncio.sleep(interval)
                
                # Placeholder –¥–ª—è –∑–±–æ—Ä—É —Å–∏–≥–Ω–∞–ª—ñ–≤
                # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—ñ —Ç—É—Ç –º–∞—î –±—É—Ç–∏ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –∑ SignalGenerator
                current_time = time.time()
                
                for symbol in settings.pairs.trade_pairs:
                    signal_data = {
                        'timestamp': current_time,
                        'symbol': symbol,
                        'signal': 'HOLD',
                        'strength': 0,
                        'composite': 0.0,
                        'imbalance': 0.0,
                        'momentum': 0.0,
                        'volatility': 0.0,
                        'settings_snapshot': json.dumps({
                            'weight_imbalance': settings.signals.weight_imbalance,
                            'weight_momentum': settings.signals.weight_momentum,
                            'hold_threshold': settings.signals.hold_threshold,
                        })
                    }
                    self._add_to_buffer('signals', symbol, signal_data)
                    
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"‚ùå [SIGNALS_LOOP] Error: {e}")
    
    def _add_to_buffer(self, buffer_type: str, symbol: str, data: Dict):
        """–î–æ–¥–∞–≤–∞–Ω–Ω—è –¥–æ –±—É—Ñ–µ—Ä–∞"""
        key = f"{symbol}_{buffer_type}"
        if key not in self.buffers[buffer_type]:
            self.buffers[buffer_type][key] = []
        
        self.buffers[buffer_type][key].append(data)
        
        # Flush —è–∫—â–æ –¥–æ—Å—è–≥–ª–∏ —Ä–æ–∑–º—ñ—Ä—É –±—É—Ñ–µ—Ä–∞
        if len(self.buffers[buffer_type][key]) >= self.buffer_size:
            self._flush_buffer(buffer_type, symbol)
    
    async def _flush_loop(self):
        """–ü–µ—Ä—ñ–æ–¥–∏—á–Ω–∏–π flush –±—É—Ñ–µ—Ä—ñ–≤"""
        while self._running:
            try:
                await asyncio.sleep(60)
                
                current_time = time.time()
                if current_time - self.last_flush >= 60:
                    self._flush_all_buffers()
                    self.last_flush = current_time
                    
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"‚ùå [FLUSH_LOOP] Error: {e}")
    
    def _flush_buffer(self, buffer_type: str, symbol: str):
        """–ó–∞–ø–∏—Å –±—É—Ñ–µ—Ä–∞ –≤ Parquet –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º ParquetWriter"""
        key = f"{symbol}_{buffer_type}"
        
        if key not in self.buffers[buffer_type] or not self.buffers[buffer_type][key]:
            return
        
        try:
            data_list = self.buffers[buffer_type][key]
            
            # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ PyArrow Table
            table = pa.Table.from_pylist(data_list, schema=self.schemas[buffer_type])
            
            # –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É
            today = datetime.utcnow().strftime("%Y-%m-%d")
            date_path = self.raw_path / today
            date_path.mkdir(parents=True, exist_ok=True)
            file_path = date_path / f"{symbol}_{buffer_type}.parquet"
            
            # –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è ParquetWriter –¥–ª—è append
            writer_key = str(file_path)
            
            if writer_key not in self.writers:
                # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–æ–≥–æ writer
                if file_path.exists():
                    # –§–∞–π–ª —ñ—Å–Ω—É—î - –≤—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ –¥–ª—è append
                    # –ß–∏—Ç–∞—î–º–æ —ñ—Å–Ω—É—é—á—ñ –¥–∞–Ω—ñ
                    existing_table = pq.read_table(file_path)
                    # –û–±'—î–¥–Ω—É—î–º–æ –∑ –Ω–æ–≤–∏–º–∏
                    combined_table = pa.concat_tables([existing_table, table])
                    # –ü–µ—Ä–µ–∑–∞–ø–∏—Å—É—î–º–æ —Ñ–∞–π–ª
                    pq.write_table(
                        combined_table,
                        file_path,
                        compression='snappy',
                        version='2.6'
                    )
                else:
                    # –ù–æ–≤–∏–π —Ñ–∞–π–ª - –ø—Ä–æ—Å—Ç–æ –∑–∞–ø–∏—Å—É—î–º–æ
                    pq.write_table(
                        table,
                        file_path,
                        compression='snappy',
                        version='2.6'
                    )
            else:
                # Writer –≤–∂–µ —ñ—Å–Ω—É—î - append
                self.writers[writer_key].write_table(table)
            
            # –û—á–∏—â–µ–Ω–Ω—è –±—É—Ñ–µ—Ä–∞
            self.buffers[buffer_type][key] = []
            
            logger.debug(f"üíæ [FLUSH] {buffer_type}/{symbol}: {len(data_list)} records")
            
        except Exception as e:
            logger.error(f"‚ùå [FLUSH] {buffer_type}/{symbol}: {e}")
            # –ù–µ –≤—Ç—Ä–∞—á–∞—î–º–æ –¥–∞–Ω—ñ –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ - –∑–∞–ª–∏—à–∞—î–º–æ –≤ –±—É—Ñ–µ—Ä—ñ
    
    def _flush_all_buffers(self):
        """Flush –≤—Å—ñ—Ö –±—É—Ñ–µ—Ä—ñ–≤"""
        for buffer_type in self.buffers.keys():
            for symbol in settings.pairs.trade_pairs:
                self._flush_buffer(buffer_type, symbol)
    
    def _close_all_writers(self):
        """–ó–∞–∫—Ä–∏—Ç—Ç—è –≤—Å—ñ—Ö ParquetWriter"""
        for writer_key, writer in self.writers.items():
            try:
                writer.close()
                logger.debug(f"‚úÖ [WRITER_CLOSE] {writer_key}")
            except Exception as e:
                logger.error(f"‚ùå [WRITER_CLOSE] {writer_key}: {e}")
        
        self.writers.clear()
    
    async def _rotation_loop(self):
        """–†–æ—Ç–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö"""
        while self._running:
            try:
                await asyncio.sleep(86400)  # 24 –≥–æ–¥–∏–Ω–∏
                
                logger.info("üîÑ [ROTATION] Starting data rotation...")
                
                # –í–∏–¥–∞–ª–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö RAW –¥–∞–Ω–∏—Ö
                self._cleanup_raw_data(days=settings.backtest.raw_data_retention_days)
                
                # –í–∏–¥–∞–ª–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö aggregated –¥–∞–Ω–∏—Ö
                self._cleanup_aggregated_data(days=settings.backtest.aggregated_data_retention_days)
                
                # –í–∏–¥–∞–ª–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö metadata
                self._cleanup_metadata(days=settings.backtest.metadata_retention_days)
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–∑–º—ñ—Ä—É
                total_size = self._check_storage_size()
                logger.info(f"üíæ [ROTATION] Total storage: {total_size:.2f} GB")
                
                if total_size > settings.backtest.max_storage_gb:
                    logger.warning(f"‚ö†Ô∏è [ROTATION] Storage limit exceeded: {total_size:.2f} GB > {settings.backtest.max_storage_gb} GB")
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"‚ùå [ROTATION] Error: {e}")
    
    def _cleanup_raw_data(self, days: int):
        """–í–∏–¥–∞–ª–µ–Ω–Ω—è RAW –¥–∞–Ω–∏—Ö —Å—Ç–∞—Ä—ñ—à–µ N –¥–Ω—ñ–≤"""
        cutoff = datetime.utcnow() - timedelta(days=days)
        
        for date_folder in self.raw_path.iterdir():
            if not date_folder.is_dir():
                continue
            
            try:
                folder_date = datetime.strptime(date_folder.name, "%Y-%m-%d")
                if folder_date < cutoff:
                    import shutil
                    shutil.rmtree(date_folder)
                    logger.info(f"üóëÔ∏è [CLEANUP] Removed RAW: {date_folder.name}")
            except Exception as e:
                logger.error(f"‚ùå [CLEANUP] {date_folder}: {e}")
    
    def _cleanup_aggregated_data(self, days: int):
        """–í–∏–¥–∞–ª–µ–Ω–Ω—è aggregated –¥–∞–Ω–∏—Ö —Å—Ç–∞—Ä—ñ—à–µ N –¥–Ω—ñ–≤"""
        cutoff = datetime.utcnow() - timedelta(days=days)
        
        for agg_file in self.agg_path.glob("*.parquet"):
            try:
                # –ü–∞—Ä—Å–∏–º–æ –¥–∞—Ç—É –∑ –Ω–∞–∑–≤–∏ —Ñ–∞–π–ª—É
                date_str = agg_file.stem.split('_')[0]
                file_date = datetime.strptime(date_str, "%Y-%m-%d")
                
                if file_date < cutoff:
                    agg_file.unlink()
                    logger.info(f"üóëÔ∏è [CLEANUP] Removed AGG: {agg_file.name}")
            except Exception as e:
                logger.error(f"‚ùå [CLEANUP] {agg_file}: {e}")
    
    def _cleanup_metadata(self, days: int):
        """–í–∏–¥–∞–ª–µ–Ω–Ω—è metadata —Å—Ç–∞—Ä—ñ—à–µ N –¥–Ω—ñ–≤"""
        cutoff = datetime.utcnow() - timedelta(days=days)
        
        for meta_file in self.meta_path.glob("*.parquet"):
            try:
                # –ü–∞—Ä—Å–∏–º–æ –¥–∞—Ç—É –∑ –Ω–∞–∑–≤–∏ —Ñ–∞–π–ª—É
                date_str = meta_file.stem.split('_')[0]
                file_date = datetime.strptime(date_str, "%Y-%m")
                
                if file_date < cutoff:
                    meta_file.unlink()
                    logger.info(f"üóëÔ∏è [CLEANUP] Removed METADATA: {meta_file.name}")
            except Exception as e:
                logger.error(f"‚ùå [CLEANUP] {meta_file}: {e}")
    
    def _check_storage_size(self) -> float:
        """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–∑–º—ñ—Ä—É —Å—Ö–æ–≤–∏—â–∞ –≤ –ì–ë"""
        total_size = 0
        
        for path in [self.raw_path, self.agg_path, self.meta_path]:
            for file in path.rglob("*"):
                if file.is_file():
                    total_size += file.stat().st_size
        
        return total_size / (1024 ** 3)
    
    async def _on_position_update(self, position):
        """Callback –ø—Ä–∏ –æ–Ω–æ–≤–ª–µ–Ω–Ω—ñ –ø–æ–∑–∏—Ü—ñ—ó"""
        try:
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –∑–∞–∫—Ä–∏—Ç—ñ –ø–æ–∑–∏—Ü—ñ—ó –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
            if position.status == "CLOSED" and hasattr(position, 'closed_timestamp'):
                trade_result = {
                    'timestamp': position.timestamp,
                    'closed_timestamp': position.closed_timestamp,
                    'symbol': position.symbol,
                    'side': position.side,
                    'entry_price': position.entry_price,
                    'exit_price': getattr(position, 'avg_exit_price', 0.0),
                    'pnl': getattr(position, 'realised_pnl', 0.0),
                    'close_reason': getattr(position, 'close_reason', 'UNKNOWN'),
                    'lifetime_sec': position.closed_timestamp - position.timestamp,
                    'stop_loss': getattr(position, 'stop_loss', 0.0),
                    'take_profit': getattr(position, 'take_profit', 0.0),
                }
                
                self._add_to_buffer('positions', position.symbol, trade_result)
        except Exception as e:
            logger.error(f"‚ùå [POSITION_UPDATE] Error: {e}")
